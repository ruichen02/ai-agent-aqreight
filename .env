# Backend
EMBEDDING_MODEL=local-384
LLM_PROVIDER=openai  # options: stub | openai | ollama
OPENAI_API_KEY= "api key here"            # if using OpenAI, set this
OLLAMA_HOST=http://ollama:11434
VECTOR_STORE=qdrant     # qdrant | memory
COLLECTION_NAME=policy_helper
CHUNK_SIZE=150
CHUNK_OVERLAP=30

# Frontend
NEXT_PUBLIC_API_BASE=http://localhost:8000
